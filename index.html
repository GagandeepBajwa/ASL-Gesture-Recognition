<!DOCTYPE HTML>
<html>

<head>
  <title>American Sign Language Translator</title>
  <meta name="description" content="website description" />
  <meta name="keywords" content="website keywords, website keywords" />
  <meta http-equiv="content-type" content="text/html; charset=windows-1252" />
  <link rel="stylesheet" type="text/css" href="style/style.css" />
</head>

<body>
  <div id="main">
    <div id="header">
      <div id="logo">
        <div id="logo_text">
          <!-- class="logo_colour", allows you to change the colour of the text -->
          <h1><a href="index.html"><span class="logo_colour">American Sign Language Translator</span></a></h1>
          <h2>ASL Fingerspeller</h2>
        </div>
      </div>
      <div id="menubar">
        <ul id="menu">
          <!-- put class="selected" in the li tag for the selected page - to highlight which page you're on -->
          <li class="selected"><a href="index.html">Home</a></li>
          <li><a href="examples.html">Datasets</a></li>
          <li><a href="page.html">Approach</a></li>
          <li><a href="contact.html">Contributors</a></li>
        </ul>
      </div>
    </div>
    <div id="content_header"></div>
    <div id="site_content">
      <p><div id="banner"></div></p>
        
	      
       <div class = 'content_container'>   
        <div id="content">
        
        <!-- insert the page content here -->
        <h1>Introduction</h1>
        <p>The only mode of offline communication for hearing-impaired persons is through sign language. Learning the sign language is convoluted and has many conventions. The aim of this project is to develop an American Sign Language translator in order to mitigate the aforementioned difficulties. Another aim of this project is to develop a hand gesture recognition system to establish human computer interaction.</p>
         
        </div>
        <div id = 'content_video'>
        
        <!-- insert your sidebar items here -->
         <iframe width="480" height="315"
         src="https://www.youtube.com/embed/GhI2VwPll0k">
         </iframe>
                
        </div>

      </div>
        <div id="content1">
        <!-- insert the page content here -->
        <h1>Datasets Used</h1>
        <p>The code was first tested with the Triesch Dataset, after which we created our own dataset, to suit our testing conditions (unideal) as well as, expand our letter-base</p>
        
          <h3>Triesch Dataset</h3> <p>Triesch dataset contains letters a,b,c and v against 3 different backgrounds as shown below.</p>
          <a href="http://tinypic.com?ref=25heao6" target="_blank"><img src="http://i65.tinypic.com/25heao6.jpg" border="0" alt="Image and video hosting by TinyPic"></a>

          <h3>User Dataset</h3><p>The created dataset contains images of letters a,b,c,d,g,i,l,v and y, recorded at different angles and at different distances wrt the camera. <br/>This variation is introduced in order to introduce tolerances for orientation and scale.
          <a href="http://tinypic.com?ref=2nu6kvk" target="_blank"><img src="http://i66.tinypic.com/2nu6kvk.jpg" border="0" alt="Image and video hosting by TinyPic"></a>      
          <h4>Data Augmentation</h4>
          <p>Left-Right Hands -> We generate a set of rotated images from the existing images to help train our model to make it more robust.<br/>Tilt/Zoom -> Generated set of images with different levels of tilt and zoom to increase the variety of the training dataset.</p>
      </div>
        <div id="content1">
        <!-- insert the page content here -->
        <h1>Approach</h1>
        <p><img src = "Approach.jpg"></p>
        
          <h3>Hand Segmentation</h3> <p>The Otsu algorithm segments the image into background and foreground. The input image is the grayscaled gaussain blurred image. This method classifies the pixels based on their grayscale pixel values and cluster them into 2 regions. We used this method for hand segmentation due to its invariance to dynamic light conditions </p>
          <p><img src = "imageSegmentation.jpg" width=80%></p>

          <h3>Feature Extraction</h3><p>Various features extracted for an image include the hull points, contour area, convexity defect coordinates, HoG features and bounding box. We particularly selected the HoG features due to clear distinction obtained for individual characters.   
          <img src = "Features.jpg" width=60% >
          <p><img src = "a2b_final.jpg" width=80%></p>
          <p><img src = "c2d_final.jpg" width=80%></p>
          
          <h4>Data Augmentation</h4>
          <p>Left-Right Hands -> We generate a set of rotated images from the existing images to help train our model to make it more robust.<br/>Tilt/Zoom -> Generated set of images with different levels of tilt and zoom to increase the variety of the training dataset.</p>
      </div>


      </div>
    </div>
    <div id="content_footer"></div>
    <div id="footer">
      <p><a href="index.html">Home</a> | <a href="examples.html">Datasets</a> | <a href="page.html">Approach</a> | <a href="another_page.html">Contributors</a></p>
      <p>Copyright &copy; textured_orbs | <a href="http://validator.w3.org/check?uri=referer">HTML5</a> | <a href="http://jigsaw.w3.org/css-validator/check/referer">CSS</a> | <a href="http://www.html5webtemplates.co.uk">Website templates</a></p>
    </div>
  </div>
</body>
</html>
